```{python}
import os
import torch
import torch.nn as nn
import torch.optim as optim
from PIL import Image
import matplotlib.pyplot as plt
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torch.nn.functional as F
from torch.utils.data import DataLoader, random_split
import numpy as np
from sklearn.model_selection import train_test_split
```

```{python}
if torch.backends.mps.is_available():
    device = torch.device('mps')
elif torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')

torch.set_default_device(device)

print(device)
```

```{python}
imsize = 64

loader = transforms.Compose([
    transforms.Resize(imsize),
    transforms.ToTensor()
])

dataset = datasets.ImageFolder(
    root='data/ThisPersonDoesNotExist/',
    transform=loader
)

train_size = int(0.8*len(dataset))
test_size = len(dataset) - train_size

train_dataset, test_dataset = random_split(
    dataset,
    [train_size, test_size],
    generator=torch.Generator(device=device)
)

train_loader = DataLoader(
    train_dataset,
    batch_size=500,
    shuffle=True,
    generator=torch.Generator(device=device)
)

test_loader = DataLoader(
    test_dataset,
    batch_size=500,
    shuffle=True,
    generator=torch.Generator(device=device)
)
```

```{python}
for i, (X_train, y_train) in enumerate(train_loader):
    print(X_train.device)
    print(y_train.device)
    break
```

```{python}
class cnnModel(nn.Module):
    def __init__(self):
        super().__init__()
        # Layer 1: 3*64*64 -> 8*62*62
        self.conv_1 = nn.Conv2d(
            in_channels=3, 
            out_channels=8, 
            kernel_size=3,
            stride=1
        )
        self.batchnorm_1 = nn.BatchNorm2d(
            num_features=8
        )
        self.maxpool_1 = nn.MaxPool2d(
            kernel_size=2, 
            stride=2
        )
        # Layer 2: 8*62*62 -> 16*29*29
        self.conv_2 = nn.Conv2d(
            in_channels=8, 
            out_channels=16, 
            kernel_size=3,
            stride=1
        )
        self.batchnorm_2 = nn.BatchNorm2d(
            num_features=16
        )
        self.maxpool_2 = nn.MaxPool2d(
            kernel_size=2,
            stride=2
        )
        # Layer 3: 16*29*29 -> 32*13*13
        self.conv_3 = nn.Conv2d(
            in_channels=16,
            out_channels=32,
            kernel_size=1,
            stride=1
        )
        self.batchnorm_3 = nn.BatchNorm2d(
            num_features=32
        )
        self.maxpool_3 = nn.MaxPool2d(
            kernel_size=2,
            stride=2
        )
        # Layer 4: 32*13*13 -> 64*7*7
        self.conv_4 = nn.Conv2d(
            in_channels=32,
            out_channels=64,
            kernel_size=3,
            stride=1,
            padding=1
        )
        self.batchnorm_4 = nn.BatchNorm2d(
            num_features=64
        )
        self.maxpool_4 = nn.MaxPool2d(
            kernel_size=2,
            stride=2,
            padding=1
        )
        # Layer 5: 64*7*7 -> 128*5*5
        self.conv_5 = nn.Conv2d(
            in_channels=64,
            out_channels=128,
            kernel_size=3,
            stride=1,
            padding=1
        )
        self.batchnorm_5 = nn.BatchNorm2d(
            num_features=128
        )
        self.maxpool_5 = nn.MaxPool2d(
            kernel_size=2,
            stride=2,
            padding=1
        )
        # Layer 6: 128*5*5 -> 256*4*4
        self.conv_6 = nn.Conv2d(
            in_channels=128,
            out_channels=256,
            kernel_size=3,
            stride=1,
            padding=1
        )
        self.batchnorm_6 = nn.BatchNorm2d(
            num_features=256
        )
        self.maxpool_6 = nn.MaxPool2d(
            kernel_size=2,
            stride=2,
            padding=1
        )
        # Layer 7: 256*4*4 -> 512*3*3
        self.conv_7 = nn.Conv2d(
            in_channels=256,
            out_channels=512,
            kernel_size=3,
            stride=1,
            padding=1
        )
        self.batchnorm_7 = nn.BatchNorm2d(
            num_features=512
        )
        self.maxpool_7 = nn.MaxPool2d(
            kernel_size=2,
            stride=2,
            padding=1
        )
        self.dropout_1 = nn.Dropout2d(
            p=0.5
        )
        # Layer 8: 512*3*3 -> 100*1*1
        self.fc_1 = nn.Linear(
            in_features=2*2*512,
            out_features=100
        )
        self.dropout_2 = nn.Dropout1d(
            p=0.5
        )
        # Layer 9: 100*1*1 -> 50*1*1
        self.fc_2 = nn.Linear(
            in_features=100,
            out_features=50
        )
        self.dropout_3 = nn.Dropout1d(
            p=0.5
        )
        # Output Layer: 50*1*1 -> 2*1*1
        self.fc_3 = nn.Linear(
            in_features=50, 
            out_features=2
        )

    def forward(self, x):
        # Layer 1
        x = self.conv_1(x)
        x = self.batchnorm_1(x)
        x = F.relu(x)
        x = self.maxpool_1(x)

        # Layer 2
        x = self.conv_2(x)
        x = self.batchnorm_2(x)
        x = F.relu(x)
        x = self.maxpool_2(x)

        # Layer 3
        x = self.conv_3(x)
        x = self.batchnorm_3(x)
        x = F.relu(x)
        x = self.maxpool_3(x)

        # Layer 4
        x = self.conv_4(x)
        x = self.batchnorm_4(x)
        x = F.relu(x)
        x = self.maxpool_4(x)

        # Layer 5
        x = self.conv_5(x)
        x = self.batchnorm_5(x)
        x = F.relu(x)
        x = self.maxpool_5(x)

        # Layer 6
        x = self.conv_6(x)
        x = self.batchnorm_6(x)
        x = F.relu(x)
        x = self.maxpool_6(x)

        # Layer 7
        x = self.conv_7(x)
        x = self.batchnorm_7(x)
        x = F.relu(x)
        x = self.maxpool_7(x)
        x = self.dropout_1(x)

        # Layer 8
        x = torch.flatten(x, 1)
        x = self.fc_1(x)
        x = F.relu(x)
        x = self.dropout_2(x)

        # Layer 9
        x = self.fc_2(x)
        x = F.relu(x)
        x = self.dropout_3(x)

        # Output Layer
        x = self.fc_3(x)
        x = F.softmax(x, dim=1)

        return x
```

```{python}
torch.manual_seed(2687)
cnn = cnnModel()
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(
    cnn.parameters(), 
    lr=0.1,
    momentum=0.9
)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer=optimizer,
    mode='min',
    factor=0.9
)

print(next(cnn.parameters()).device)
```

```{python}
epochs = 5
train_losses = []
test_losses = []
train_correct = []
test_correct = []
for i in range(epochs):
    train_cr = 0
    test_cr = 0

    for j, (X_train, y_train) in enumerate(train_loader):
        j += 1
        X_train = X_train.to(device)
        y_pred = cnn.forward(X_train)
        loss = criterion(y_pred, y_train)

        predicted = torch.max(y_pred.data, 1)[1]
        batch_cr = (predicted == y_train).sum()
        train_cr += batch_cr

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        print(f'Epoch: {i+1}  Batch: {j}  Loss: {loss.item()}')
    
    train_losses.append(loss)
    train_correct.append(train_cr)

    with torch.no_grad():
        for j, (X_test, y_test) in enumerate(test_loader):
            X_test = X_test.to(device)
            y_val = cnn.forward(X_test)
            predicted = torch.max(y_val.data, 1)[1]
            test_cr += (predicted == y_test).sum()

    loss = criterion(y_val, y_test)
    scheduler.step(loss)
    test_losses.append(loss)
    test_correct.append(test_cr)
```

```{python}
```

```{python}
with torch.no_grad():
    y_eval = cnn.forward(X_test)
    loss = criterion(y_eval, y_test)

loss
```